#+TITLE: Legacy branch

*The code on this branch is not developed.
This branch is kept only
to keep old work easily accessible
but out of the way.*

A number of tools and functions
to do hierarchical domain decomposition
of hypercubic lattices
and managing the memory layout.

*Nothing clever or revolutionary here*.

In this document the main ideas are discussed,
the [[implementation.org][implementation]] is described separately.

* Main Ideas
Hypercubic lattices have
"partitioning" symmetries
that constrain the
most general memory layout that can be sensibly adopted.

By "partitioning symmetry",
I refer to the fact that if we partition
#+begin_src
`----------------`
#+end_src
into
#+begin_src
`|----|----|----|----|`
#+end_src
each one of the 4 chunk has the same shape.

HPC machines tend to have
a more and more hierarchical
memory structure, for example:
- vector lanes
- core-specific caches
- socket-specific caches
- node ram
- the whole memory used
  by a distributed data structure
  (which is only an abstraction,
  that makes anyway sense)
- whatever else vendors may come up with.

Of course, such memory structure
can break the "partitioning" symmetries of the lattice,
so one might have to adopt memory layouts
that do not respect
such symmetries.
The point is:
how much do we need
to break the symmetry
to have an efficient implementation?
The less we break it,
the easier the code will be
to write
and to understand once it's written.

* Definition of memory layout
A memory layout for a lattice
is a function that takes the coordinates

#+begin_src
x_0, ... , x_d
#+end_src

of a point
and returns a natural number:

#+begin_src
idx = f ( x_0, ... , x_d )
#+end_src

A hypercubic lattice has also
global dimensions

#+begin_src
L_0, ... , L_d
#+end_src

A memory layout function
allows us to completely abstract
the problem of finding lattice sites in memory.
It can either be used on-line,
or the results can be cached.


** Decomposition of dimensions in factors

We can decompose all the dimensions
as a product of factors:

#+begin_src
L_0 = F_00 * ... * F_0(n_0-1)
...
L_d = F_d0 * ... * F_d(n_d-1)
#+end_src

For the sake of argument,
and to support following examples,
let's assume:

#+begin_src
L_0 = 32 = 2 * 2 * 4 * 2
L_1 = 27 = 3 * 9
L_2 = 35 = 5 * 7
#+end_src


** Express coordinates using the decomposition factors

We can re-express
the individual components of the coordinates
using the decomposition factors
of the relative dimension,
in a way similar to the one used
in positional numeral systems.

For the sake of argument,
using the previous decomposition of L_0,
let's assume:

#+begin_src
x_0 = 25 = 1 * 1 +
           0 * 1 * 2 +
           1 * 1 * 2 * 2
           1 * 1 * 2 * 2 * 4
#+end_src

The we can represent x_0 as follows:

#+begin_src
x_0 <--> (x_00,x_01,x_02,x_03)
x_0 <--> (1,0,1,1)
#+end_src

using the /strides/

#+begin_src
s_00 = 1
s_01 = 1 * 2
s_02 = 1 * 2 * 2
s_03 = 1 * 2 * 2 * 4
#+end_src

Where obviously

#+begin_src
s_i0 = 1
s_ij = s_i(j-1) * F_i(j-1)
#+end_src

Let's choose also x_1 and x_2

#+begin_src
x_1 = 14 = 2 * 1 +
           4 * 1 * 3
#+end_src


#+begin_src
x_2 = 26 = 1 * 1 +
           5 * 1 * 5
#+end_src


We can then express the lattice point

#+begin_src
x = ( x_0, x_1, x_2 )
#+end_src

as
#+begin_src
x = ((1, 0, 1, 1),
     (2, 4),
     (1, 5))
#+end_src

** Lexicographic memory layout

A possible memory layout is the following:

#+begin_src
idx = x_0 * 1 +
      x_1 * 1 * L_0 +
      x_2 * 1 * L_0 * L_1 +
      x_2 * 1 * L_0 * L_1 * L_2
#+end_src

which can be expressed as the list

#+begin_src
[L_0,L_1,L_2,L_3]
#+end_src

We can define the memory layout
to be the function f such that

#+begin_src
idx = f([L_0,L_1,L_2,L_3],[x_0,x_1,x_2,x_3])
#+end_src

Where ~0 <= x_d < L_d~.

Another completely equivalend memory layout is

#+begin_src
idx = x_00*1 +             // From L_0
      x_01*1*2 +           //
      x_02*1*2*2 +         //
      x_03*1*2*2*4 +       //
      //
      x_10*1*2*2*4*2 +     // From L_1
      x_11*1*2*2*4*2*3 +   //
      //
      x_20*1*2*2*4*2*3*9 + // From L_2
      x_21*1*2*2*4*2*3*9*5 //
      //    | L_0   |L_1|L_2 |
#+end_src


Which can be expressed as

#+begin_src
idx = f([F_00, F_01, F_02, F_03, F_10, F_11, F_20, F_21],
        [x_00, x_01, x_02, x_03, x_10, x_11, x_20, x_21])
#+end_src
Where ~0 <= x_ab < F_ab~.

Some statements:

1. /if we apply the same permutation
    to the list of Fs
    and to the list of xs,
    we get a valid memory layout/.
2. /permuting the lists as described
    can be seen as a tensor index transposition/.
3. /the memory layout functions
    can be made modular/.

** Truncated and reduced memory layouts

I decide to use these two adjectives
for two complementary concepts:

*** Truncated memory layout

Be ~f~ being the full memory layout
#+begin_src
idx = f([F_00, F_01, F_02, F_03, F_10, F_11, F_20, F_21],
        [x_00, x_01, x_02, x_03, x_10, x_11, x_20, x_21])
#+end_src
and equal to the "0-truncated" memory layout,
then the 5-truncated memory layout is
#+begin_src
idx = f([F_00, F_01, F_02, F_03, F_10, F_11, F_20, F_21],
        [   0,    0,    0, x_03, x_10, x_11, x_20, x_21])
#+end_src
since it does not depend on x_00, x_01 and x_02 any more,
we can compute it as
#+begin_src
idx = f([F_00* F_01* F_02, F_03, F_10, F_11, F_20, F_21],
        [               0, x_03, x_10, x_11, x_20, x_21]) .
#+end_src
The value of ~idx~ computed in this way
represents the start of a full partition being indexed,
as opposed to a general point.

*** Reduced memory layout (for homogeneous partitioning)
Be ~f~ being the full memory layout
#+begin_src
idx = f([F_00, F_01, F_02, F_03, F_10, F_11, F_20, F_21],
        [x_00, x_01, x_02, x_03, x_10, x_11, x_20, x_21])
#+end_src
and equal to the "0-reduced" memory layout,
the 5-reduced memory layout is
#+begin_src
idx = f([F_00, F_01, F_02, F_03, F_10, F_11, F_20, F_21],
        [x_00, x_01, x_02,    0,    0,    0,    0,    0])
#+end_src

In the case of inhomogeneous partitioning,
F_00, F_01, and F_02 would actually depend
on x_03 and the other "slow" variables,



** Truncated and reduced memory layouts

If the partitioning is homogeneous, then
the full memory layout is just the sum
of the N-truncated memory layout
and the N-reduced memory layout.
#+begin_src
f([F_00, F_01, F_02, F_03, F_10, F_11, F_20, F_21],
  [x_00, x_01, x_02, x_03, x_10, x_11, x_20, x_21]) =
f([F_00* F_01* F_02, F_03, F_10, F_11, F_20, F_21],
  [               0, x_03, x_10, x_11, x_20, x_21]) +
f([F_00, F_01, F_02, F_03, F_10, F_11, F_20, F_21],
  [x_00, x_01, x_02,    0,    0,    0,    0,    0])
#+end_src

* Breaking the symmetry - From ND-array to tree

There are a number of features
that are needed in order
to make a memory layout useful:
- even/odd partitioning
- the possibility to split the regions
  into bulk, border and halos
- optionally, the possibilty of
  inhomogeneous partitioning,
  i.e., having some partitions
  that are smaller than others.

Homogeneous hierarchical partitioning
produces hypercubic arrays,
which
(as we discussed)
are trivial to transpose
and should be preferred.

Inhomogeneous partitioning produces instead
/ragged/ arrays,
which are non trivial to transpose,
and are better represented as trees.

The most general structure
would be a directed acyclic graph
where the nodes are the levels,
and the arcs the dependences between them.
By dependence,
I mean the fact that the range for an index
at a certain level depends ~b~ on
the values of the indices at another other level
~a~, or potentially on more than one leve.


The implementation could decide
not to transpose the arrays.
In case this is needed,
a possible technique
to transpose such array is
padding and masking.

** EO ordering
Each portion of a lattice
can be divided into even and odd sites.
If the global lattice extents
are even in the dimensions
in which the boundary conditions are periodic,
then the lattice is a bipartite graph.

** Halos, Borders, and bulk
Splitting each portion of a lattice
into halos, borders and bulk
obviously requires
a more complex memory layout,
because it breaks the partitioning symmetry.

The situation is:
- For Local data
  Each direction can be split into 3 pieces:
  - Border-,
  - Bulk,
  - Border+
- For Cached Remote data
  - Halo-
  - Halo+
The size of Borders and Halos can be zero.

We can recover the symmetry
at a higher level,
by splitting each 1D portion in 5 pieces.
We have then at least an elegant & simple way
to refer to each portion.
This can be done at each level in the decomposition.

*How this plays with the hierarchical aspect
still requires some investigation/ideas*.

Notice that this requires having up to 5^D portions,
and this might be impractical.

We can, though, allocate only
the ones that we are interested in
by defining them all and then
filtering out the ones we do not want,
according to different requirements,
e.g.:
- having size > 0
- having at least nd_min sides > 1

For each level in the decomposition,
or for each stride,
we have a halo thickness h.

Example:

#+begin_src
For L_0 = 2 * 2 * 4 * 2,
#+end_src

We can have

#+begin_src
s_00 = 1 , h_00 = 0
s_01 = 2 , h_01 = 0
s_02 = 4 , h_02 = 1
s_03 = 16, h_03 = 1
#+end_src

Notice that if i<j, h_ki <= h_kj.


** Inhomogeneous partitioning

Requiring the dimension of the lattice
to have certain factors can be too restrictive.
In HiRep it is possible to have inhomogeneous MPI partitioning,
and it should be possible to replicate this
in a hierarchical way.

